{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import compress\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from plotly import graph_objects\n",
    "from plotly import offline as plotly_offline\n",
    "\n",
    "from scrapi.dataset import Gene_Expression_Dataset as GED\n",
    "from scrapi import plotting as scrap_plotting\n",
    "\n",
    "from pepars.plotting import plotting\n",
    "plotting.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sample to perform debris filtering on\n",
    "SAMPLE = \"AM1\"\n",
    "\n",
    "# Pick a cell number range. The first number should be a number higher than how many cells you expect.\n",
    "# The second number should be as low as possible, such that you are confident that you will still get\n",
    "# all of the cell types you're interested in\n",
    "CELL_RANGE = [12000, 1000]\n",
    "\n",
    "# How many sources of noise with unique transcript count profile you expect\n",
    "# If you only expect empty droplets, this should be 1. If you expect empty droplets + debris, 2\n",
    "NUM_SOURCES_OF_NOISE_EXPECTED = 3\n",
    "\n",
    "# How many cell types you expect at the top level of your hierarchy.\n",
    "NUM_CELL_TYPES_EXPECTED = 4\n",
    "\n",
    "# How many clusters to try to separate into. Recommend at least num sources of noise, plus\n",
    "# each major cell type, plus each subtype one level below major. For blood, we do\n",
    "# noise (3) + cell types (4) + cell subtypes (2*cell types = 8) = 15\n",
    "MAX_NUM_CLUSTERS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ged = GED(os.path.join(\"data\", SAMPLE))\n",
    "\n",
    "# Grab the transcript count matrix and total transcript count vector - we'll be using this a lot\n",
    "transcript_counts = ged.get_cell_transcript_counts()\n",
    "total_transcript_counts = ged.get_cell_total_transcript_counts()\n",
    "\n",
    "min_num_clusters = NUM_SOURCES_OF_NOISE_EXPECTED + NUM_CELL_TYPES_EXPECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the transcript vs cell count plot\n",
    "\n",
    "scrap_plotting.plot_barcode_transcript_counts(\n",
    "    total_transcript_counts,\n",
    "    max_points=1000,\n",
    "    elbow=False,\n",
    "    interactive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the transcript counts to get our transcript count thresholds based on our cell range estimate\n",
    "\n",
    "sorted_transcript_counts = numpy.sort(total_transcript_counts)\n",
    "transcript_count_thresholds = int(sorted_transcript_counts[-CELL_RANGE[0]]), int(sorted_transcript_counts[-CELL_RANGE[1]])\n",
    "debris_transcript_count_threshold = transcript_count_thresholds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of cells above the threshold\n",
    "cell_indices_above_threshold = total_transcript_counts > debris_transcript_count_threshold\n",
    "\n",
    "# Get the full gene count matrix and total transcript counts associated with these cells\n",
    "filtered_cell_gene_counts = transcript_counts[cell_indices_above_threshold, :]\n",
    "filtered_total_transcript_counts = total_transcript_counts[cell_indices_above_threshold]\n",
    "\n",
    "print(\"%i cell(s) after threshold filtering\" % filtered_cell_gene_counts.shape[0])\n",
    "\n",
    "# Filter out zero genes to make it easier on PCA\n",
    "non_zero_genes = filtered_cell_gene_counts.sum(axis=0) > 0\n",
    "filtered_cell_gene_counts = filtered_cell_gene_counts[:, non_zero_genes]\n",
    "\n",
    "print(\"%i gene(s) after zero filtering\" % filtered_cell_gene_counts.shape[1])\n",
    "\n",
    "# Normalize the cell gene counts - first divide by the sum to get transcripts per cell\n",
    "filtered_cell_gene_counts.divide(filtered_cell_gene_counts.sum(axis=1))\n",
    "\n",
    "# Multiply by a factor to separate single counts from zero counts\n",
    "filtered_cell_gene_counts.multiply(5000)\n",
    "\n",
    "# Add one before taking log\n",
    "filtered_cell_gene_counts.add(1)\n",
    "\n",
    "# Log scale\n",
    "filtered_cell_gene_counts.log10()\n",
    "\n",
    "print(\"Dimensionality reduction via PCA\")\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "transformed_PCA = pca.fit_transform(filtered_cell_gene_counts.to_array())\n",
    "\n",
    "print(\"Dimensionality reduction via tSNE\")\n",
    "\n",
    "transformed_tSNE = TSNE(\n",
    "    verbose=True, perplexity=30, n_components=2, n_jobs=4).\\\n",
    "    fit_transform(transformed_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt agglomerative clustering for a range of clusters and find the highest one\n",
    "\n",
    "cluster_range = range(min_num_clusters, MAX_NUM_CLUSTERS)\n",
    "\n",
    "silhouette_scores = []\n",
    "\n",
    "for num_clusters in cluster_range:\n",
    "\n",
    "    print(\"Testing %i clusters\" % num_clusters)\n",
    "\n",
    "    clusterer = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "    clusters = clusterer.fit_predict(transformed_PCA)\n",
    "\n",
    "    silhouette_score = metrics.silhouette_score(transformed_PCA, clusters)\n",
    "    silhouette_scores.append(silhouette_score)\n",
    "\n",
    "num_clusters = cluster_range[numpy.argmax(silhouette_scores)]\n",
    "\n",
    "plotting.plot_scatter(\n",
    "    list(cluster_range),\n",
    "    silhouette_scores,\n",
    "    interactive=True,\n",
    "    title=\"Silhouette distance for threshold %i\" % debris_transcript_count_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the highest number of clusters found and proceed\n",
    "\n",
    "clusterer = AgglomerativeClustering(n_clusters=num_clusters)\n",
    "clusters = clusterer.fit_predict(transformed_PCA)\n",
    "\n",
    "x_values = []\n",
    "y_values = []\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "\n",
    "    cluster_points = transformed_tSNE[clusters == cluster, :]\n",
    "\n",
    "    x_values.append(cluster_points[:, 0])\n",
    "    y_values.append(cluster_points[:, 1])\n",
    "\n",
    "plotting.plot_scatter(\n",
    "    x_values,\n",
    "    y_values,\n",
    "    interactive=True,\n",
    "    title=\"Clustering for transcript count threshold %i\" % debris_transcript_count_threshold,\n",
    "    trace_names=list(range(num_clusters))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mitochondrial gene ratio (for QC)\n",
    "\n",
    "mt_genes = []\n",
    "\n",
    "for gene in ged.get_genes():\n",
    "    if gene.lower().startswith(\"mt-\"):\n",
    "        mt_genes.append(gene)\n",
    "        \n",
    "mt_ratio = transcript_counts[cell_indices_above_threshold, mt_genes].sum(axis=1)/\\\n",
    "    transcript_counts[cell_indices_above_threshold, :].sum(axis=1)\n",
    "\n",
    "figure = graph_objects.Figure(data=graph_objects.Scatter(\n",
    "    x=transformed_tSNE[:, 0],\n",
    "    y=transformed_tSNE[:, 1],\n",
    "    mode=\"markers\",\n",
    "    text=mt_ratio,\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=mt_ratio,\n",
    "        colorscale=\"Viridis\",\n",
    "        showscale=True\n",
    "    )\n",
    "))\n",
    "\n",
    "plotly_offline.iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the HBB ratio (for QC)\n",
    "        \n",
    "HBB_ratios = transcript_counts[cell_indices_above_threshold, \"HBB\"].sum(axis=1)/\\\n",
    "    transcript_counts[cell_indices_above_threshold, :].sum(axis=1)\n",
    "\n",
    "figure = graph_objects.Figure(data=graph_objects.Scatter(\n",
    "    x=transformed_tSNE[:, 0],\n",
    "    y=transformed_tSNE[:, 1],\n",
    "    mode=\"markers\",\n",
    "    text=HBB_ratios,\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=HBB_ratios,\n",
    "        colorscale=\"Viridis\",\n",
    "        showscale=True\n",
    "    )\n",
    "))\n",
    "\n",
    "plotly_offline.iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transcript counts (for QC)\n",
    "        \n",
    "transcript_count_sums = transcript_counts[cell_indices_above_threshold, :].sum(axis=1)\n",
    "\n",
    "figure = graph_objects.Figure(data=graph_objects.Scatter(\n",
    "    x=transformed_tSNE[:, 0],\n",
    "    y=transformed_tSNE[:, 1],\n",
    "    mode=\"markers\",\n",
    "    text=transcript_count_sums,\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=transcript_count_sums,\n",
    "        colorscale=\"Viridis\",\n",
    "        showscale=True\n",
    "    )\n",
    "))\n",
    "\n",
    "plotly_offline.iplot(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a range of thresholds to test from the min to the max - this is to establish a slope of cell\n",
    "# dropoff per cluster as we increase the transcript count threshold\n",
    "fine_grain_transcript_count_thresholds = range(debris_transcript_count_threshold, transcript_count_thresholds[1], 50)\n",
    "\n",
    "# A dataframe that lists the number of cells in each cluster above the thresholds\n",
    "cluster_size_from_most_to_least = pandas.DataFrame(\n",
    "    index=list(range(num_clusters)),\n",
    "    columns=list(fine_grain_transcript_count_thresholds)\n",
    ")\n",
    "\n",
    "for threshold in fine_grain_transcript_count_thresholds:\n",
    "\n",
    "    for cluster in range(num_clusters):\n",
    "        \n",
    "        # Get how many cells are in this cluster at this threshold\n",
    "        cluster_cell_counts = \\\n",
    "            filtered_total_transcript_counts[(clusters == cluster) & (filtered_total_transcript_counts > threshold)]\n",
    "\n",
    "        cluster_size_from_most_to_least.loc[cluster, threshold] = cluster_cell_counts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect to see the cell dropoff curve for the different clusters\n",
    "\n",
    "plotting.plot_scatter(\n",
    "    [cluster_size_from_most_to_least.columns]*num_clusters,\n",
    "    cluster_size_from_most_to_least.values,\n",
    "    interactive=True,\n",
    "    trace_names=list(range(num_clusters)),\n",
    "    x_axis_title=\"Transcript Count Threshold\",\n",
    "    y_axis_title=\"# of cells remaining in cluster\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transcript_count_threshold = transcript_count_thresholds[0]\n",
    "\n",
    "# Calculate the cell loss (% cells remaining) at each threshold to make a normalized trace\n",
    "final_cluster_loss = cluster_size_from_most_to_least.iloc[:,-1]/cluster_size_from_most_to_least.iloc[:,0]\n",
    "cluster_losses = cluster_size_from_most_to_least.values[:, 1:]/cluster_size_from_most_to_least.values[:, 0].reshape((-1, 1))\n",
    "cluster_losses[numpy.isnan(cluster_losses)] = 0\n",
    "\n",
    "# Cluster the traces into two - noise and not\n",
    "noise_clusterer = AgglomerativeClustering(n_clusters=2)\n",
    "noise_clusters = noise_clusterer.fit_predict(cluster_losses)\n",
    "\n",
    "# Find the cluster that maintains the largest number of cells - this is our signal\n",
    "signal_cluster = noise_clusters[numpy.where(final_cluster_loss.values == final_cluster_loss.values.max())[0][0]]\n",
    "\n",
    "# Initialize a boolean array with all false\n",
    "valid_cells = numpy.array([False]*filtered_cell_gene_counts.num_rows)\n",
    "\n",
    "print(\"Filtering out cluster(s): %s\" % str(numpy.argwhere(noise_clusters != signal_cluster)[:, 0]))\n",
    "print(\"Keeping cluster(s): %s\" % str(numpy.argwhere(noise_clusters == signal_cluster)[:, 0]))\n",
    "\n",
    "# Loop through all clusters and mark any cells that are part of a valid cluster as valid\n",
    "for cluster in range(num_clusters):\n",
    "    \n",
    "    if noise_clusters[cluster] == signal_cluster:\n",
    "        valid_cells = valid_cells | (clusters == cluster)\n",
    "        \n",
    "cluster_filtered_cell_gene_counts = filtered_cell_gene_counts[valid_cells, :]\n",
    "\n",
    "print(\"Filtered out %i cells out of %i\" % (len(valid_cells) - valid_cells.sum(), len(valid_cells)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
